{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6521, 11)\n",
      "Size before: (450668, 13)\n",
      "Size after: (72458, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "movie_metadata = pd.read_csv('MovieSummaries/movie.metadata.tsv',sep='\\t')\n",
    "\n",
    "movie_metadata.columns = ['1. Wikipedia movie ID',\n",
    "                          '2. Freebase movie ID',\n",
    "                          '3. Movie name',\n",
    "                          '4. Movie release date',\n",
    "                          '5. Movie box office revenue',\n",
    "                          '6. Movie runtime',\n",
    "                          '7. Movie languages (Freebase ID:name tuples)',\n",
    "                          '8. Movie countries (Freebase ID:name tuples)',\n",
    "                          '9. Movie genres (Freebase ID:name tuples)']\n",
    "\n",
    "character_metadata = pd.read_csv('MovieSummaries/character.metadata.tsv',sep='\\t')\n",
    "\n",
    "character_metadata.columns = ['1. Wikipedia movie ID',\n",
    "                              '2. Freebase movie ID',\n",
    "                              '3. Movie release date',\n",
    "                              '4. Character name',\n",
    "                              '5. Actor date of birth',\n",
    "                              '6. Actor gender',\n",
    "                              '7. Actor height (in meters)',\n",
    "                              '8. Actor ethnicity (Freebase ID)',\n",
    "                              '9. Actor name',\n",
    "                              '10. Actor age at movie release',\n",
    "                              '11. Freebase character/actor map ID',\n",
    "                              '12. Freebase character ID',\n",
    "                              '13. Freebase actor ID']\n",
    "\n",
    "movie_metadata_bechdel = pd.read_csv(\"CMU_bechdel_added.csv\")\n",
    "print(movie_metadata_bechdel.shape)\n",
    "movie_metadata_bechdel = movie_metadata_bechdel.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "\n",
    "character_metadata_bechdel = character_metadata.copy(deep = True)\n",
    "print(\"Size before:\", character_metadata_bechdel.shape)\n",
    "character_metadata_bechdel = character_metadata_bechdel[character_metadata_bechdel['2. Freebase movie ID'].isin(movie_metadata_bechdel[\"2. Freebase movie ID\"].to_numpy())]\n",
    "print(\"Size after:\", character_metadata_bechdel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6521, 10)\n",
      "(6202, 10)\n"
     ]
    }
   ],
   "source": [
    "print(movie_metadata_bechdel.shape)\n",
    "movie_metadata_bechdel = movie_metadata_bechdel[movie_metadata_bechdel['2. Freebase movie ID'].isin(character_metadata_bechdel[\"2. Freebase movie ID\"].to_numpy())]\n",
    "print(movie_metadata_bechdel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42303, 2)\n",
      "(5194, 2)\n"
     ]
    }
   ],
   "source": [
    "plot_summaries=pd.read_csv('MovieSummaries/plot_summaries.txt', sep='\\t', header=None, names=['id', 'plot_summary'])\n",
    "plot_summaries_bechdel = plot_summaries[plot_summaries['id'].isin(character_metadata_bechdel['1. Wikipedia movie ID'].to_numpy())]\n",
    "\n",
    "plot_summaries.head()\n",
    "print(plot_summaries.shape)\n",
    "print(plot_summaries_bechdel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_idx = 255\n",
    "\n",
    "def calculate_actor_mention_score(movie_idx):\n",
    "    movie_summary = plot_summaries_bechdel.iloc[movie_idx][\"plot_summary\"]\n",
    "    \n",
    "    #Tokenize the movie summary\n",
    "    tokens = tokenizer.tokenize(movie_summary)\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    tokens_freq = pd.Series(tokens).value_counts(sort=True)\n",
    "    \n",
    "    #align movie dataset & character dataset\n",
    "    movie_id = plot_summaries_bechdel.iloc[movie_idx][\"id\"]\n",
    "    character_list = character_metadata_bechdel[character_metadata_bechdel['1. Wikipedia movie ID'] == movie_id][['4. Character name','6. Actor gender']]\n",
    "    \n",
    "    \"\"\"\n",
    "    character_list_processed = character_list.copy()\n",
    "    character_list_processed = character_list_processed.dropna()\n",
    "    \n",
    "    #Lowercase character names\n",
    "    character_list_processed[\"4. Character name\"] = character_list_processed[\"4. Character name\"].str.lower()\n",
    "\n",
    "    #Split name and surname to two columns\n",
    "    character_list_processed[[\"4. Character name\", \"4.5. Character surname\"]] = character_list_processed[\"4. Character name\"].str.split(' ', n=1, expand=True)\n",
    "\n",
    "    #Stack two columns as rows as if surnames are different characters\n",
    "    character_list_stacked = character_list_processed[[\"4. Character name\", \"4.5. Character surname\"]].stack()\n",
    "\n",
    "    #Duplicate and stack gender column so that dataframe matches\n",
    "    character_gender_stacked = pd.concat([character_list_processed[\"6. Actor gender\"],character_list_processed[\"6. Actor gender\"]])\n",
    "    character_gender_stacked = pd.concat([pd.Series(character_list_stacked.to_numpy()), pd.Series(character_gender_stacked.to_numpy())], axis = 1)\n",
    "    character_gender_stacked_idx = character_gender_stacked.set_index(0)\n",
    "    \n",
    "    #Take the intersection between the token's frequency and movie cast\n",
    "    tokens_intersection = tokens_freq[character_gender_stacked_idx.index.intersection(tokens_freq.index)]\n",
    "    character_mention_freq = character_gender_stacked[character_gender_stacked[0].isin(character_gender_stacked_idx.index.intersection(tokens_freq.index))]\n",
    "\n",
    "    #Add the number of character mentions in summary to the character meta-dataset\n",
    "    character_mention_freq[\"no_mention\"] = tokens_intersection.values#pd.DataFrame({'4. Character name':character_mention_freq[0], 'no_mention':character_mention_freq.values})\n",
    "    character_mention_freq.columns = [\"character_name\", \"gender\", \"no_mention\"]\n",
    "    character_list_final = character_mention_freq\n",
    "    #print(character_list_final)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    character_list_processed = character_list.copy()\n",
    "    character_list_processed = character_list_processed.dropna()\n",
    "\n",
    "    #Lowercase character names\n",
    "    character_list_processed[\"4. Character name\"] = character_list_processed[\"4. Character name\"].str.lower()\n",
    "\n",
    "    #Split full name and only get the first name\n",
    "    character_list_processed[\"4. Character name\"] = character_list_processed[\"4. Character name\"].str.split(' ').str[0]\n",
    "\n",
    "    character_list_processed = character_list_processed.drop(character_list_processed[character_list_processed[\"4. Character name\"] == \"the\"].index)\n",
    "\n",
    "    character_gender_stacked = character_list_processed.drop_duplicates(subset='4. Character name', keep=False)\n",
    "    character_gender_stacked_idx = character_gender_stacked.set_index(\"4. Character name\")\n",
    "\n",
    "    #Take the intersection between the token's frequency and movie cast\n",
    "    tokens_intersection = tokens_freq[character_gender_stacked_idx.index.intersection(tokens_freq.index)]\n",
    "    character_gender_stacked = character_gender_stacked.drop_duplicates()\n",
    "    character_mention_freq = character_gender_stacked[character_gender_stacked[\"4. Character name\"].isin(tokens_intersection.index)]#.drop_duplicates()\n",
    "\n",
    "    #Add the number of character mentions in summary to the character meta-dataset\n",
    "    character_mention_freq[\"no_mention\"] = tokens_intersection.values#pd.DataFrame({'4. Character name':character_mention_freq[0], 'no_mention':character_mention_freq.values})\n",
    "    character_mention_freq.columns = [\"character_name\", \"gender\", \"no_mention\"]\n",
    "    character_list_final = character_mention_freq\n",
    "    #print(character_list_final)\n",
    "    \n",
    "    #Group by gender and calculate total number of mentions by gender\n",
    "    character_list_freq_added = character_list_final.groupby(['gender']).sum()\n",
    "    if len(character_list_freq_added['no_mention'].index) != 0:\n",
    "        if character_list_freq_added['no_mention'].shape[0] == 2:\n",
    "            female_mention, male_mention = character_list_freq_added['no_mention'][0], character_list_freq_added['no_mention'][1] #groupby is alphabethic, index 0 = F\n",
    "            mention_ratio = female_mention/(female_mention + male_mention)\n",
    "        elif character_list_freq_added['no_mention'].index[0] == \"M\":\n",
    "            mention_ratio = 0.\n",
    "        elif character_list_freq_added['no_mention'].index[0] == \"F\":\n",
    "            mention_ratio = 1.\n",
    "        else:\n",
    "            mention_ratio = np.nan\n",
    "    else:\n",
    "        mention_ratio = np.nan\n",
    "\n",
    "    actor_mention_score = round(mention_ratio, 4)\n",
    "    \n",
    "    return actor_mention_score, movie_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.853267192840576\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "beginning = time.time()\n",
    "for a in range(1,1000):\n",
    "    #print(a)\n",
    "    _,_ = calculate_actor_mention_score(a)\n",
    "\n",
    "print(time.time() - beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pompous paleontologist Rick Marshall  has a low-level job at the La Brea Tar Pits, three years after a disastrous interview with Matt Lauer of Today became a viral video and ruined his career. Doctoral candidate student Holly Cantrell  tells him that his controversial theories combining time warps and paleontology inspired her. She shows him a fossil with an imprint of a cigarette lighter that he recognizes as his own along with a crystal made into a necklace that gives off strong tachyon energy. She convinces him to finish his tachyon amplifier and come help her on a seemingly routine expedition to the cave where Holly found the fossil, which is in the middle of nowhere. With cave gift shop owner Will Stanton ([[Danny McBride  they raft into the cave, where Marshall has detected high levels of tachyons. He activates the tachyon amplifier, triggering an earthquake that opens a time warp into which the raft falls. The group finds themselves in a desert, filled with various items from many eras, and without the amplifier. They rescue a primate-like creature, Cha-Ka  of the Pakuni tribe, who becomes their friend and guide. The gang spends a night in a cave after surviving a meeting with a fast, intelligent Tyrannosaurus they nickname \"Grumpy\", who develops a vendetta against Marshall for calling him stupid. Marshall receives a telepathic message begging for help and ends up in ancient ruins. There, the group encounters a race of lizard men called Sleestaks before meeting the one who sent Marshall the telepathic message, Enik the Altrusian. He explains that he was exiled by the evil Zarn who is attempting to take over Earth with his Sleestak minions, but Enik can prevent this if Marshall retrieves the tachyon amplifier. The group stumble upon a desert where many things from across time end up and they encounter many Compsognathus, Velociraptors, Grumpy, and an Allosaurus. The Allosaurus and Grumpy battle it out over the most recent thing to appear until they sense Marshall and chase him. Marshall kills the Allosaurus with liquid nitrogen and finds that the amplifier was inside the Allosaurus. The amplifier is stolen by a Pteranodon and taken to its nest. The group arrives at the nest and Marshall lightly steps through the Pteranadon eggs to retrieve the amplifier, but when he reaches it, it stops broadcasting the soundtrack to Marshalls favorite musical A Chorus Line. When the eggs begin to hatch, Holly realizes that the music was acting as a sort of lullaby keeping the Pteranadons asleep. Marshall, Will and Holly belt out \"I Hope I Get It\", with Cha-ka inexplicably joining in, displaying an impressive singing voice. Marshall, Will and Cha-ka celebrate their good fortune. Meanwhile, Holly pockets a dinosaur egg and learns from a recording left by the long-deceased Zarn that Enik deceived them and is actually the one planning to invade Earth, but is captured by the Sleestaks to be brought to the Library of Skulls for judgment. The others save her from being executed for helping Enik, but the villain&mdash;now possessing the amplifier, and mind-controlling the Sleestaks&mdash;leaves them to open a portal to Earth. Marshall quickly settles things with Grumpy, befriending him, and joins the others to defeat the Sleestak army and confront Enik. After the crystal link between the Land of the Lost and Earth is shattered, Will chooses to stay behind to live a better life and to prevent Enik from following Marshall and Holly back to Earth, learning later that female Pakuni are very attractive. A triumphant Marshall again appears on Today with show the dinosaur egg Holly brought back to promote his new book Matt Lauer Can Suck It. However, left behind on the Today set, the egg turns out to be a Sleestak egg as a baby hatches from it.\n",
      "['pompous', 'paleontologist', 'rick', 'marshall', 'has', 'a', 'low', 'level', 'job', 'at', 'the', 'la', 'brea', 'tar', 'pits', 'three', 'years', 'after', 'a', 'disastrous', 'interview', 'with', 'matt', 'lauer', 'of', 'today', 'became', 'a', 'viral', 'video', 'and', 'ruined', 'his', 'career', 'doctoral', 'candidate', 'student', 'holly', 'cantrell', 'tells', 'him', 'that', 'his', 'controversial', 'theories', 'combining', 'time', 'warps', 'and', 'paleontology', 'inspired', 'her', 'she', 'shows', 'him', 'a', 'fossil', 'with', 'an', 'imprint', 'of', 'a', 'cigarette', 'lighter', 'that', 'he', 'recognizes', 'as', 'his', 'own', 'along', 'with', 'a', 'crystal', 'made', 'into', 'a', 'necklace', 'that', 'gives', 'off', 'strong', 'tachyon', 'energy', 'she', 'convinces', 'him', 'to', 'finish', 'his', 'tachyon', 'amplifier', 'and', 'come', 'help', 'her', 'on', 'a', 'seemingly', 'routine', 'expedition', 'to', 'the', 'cave', 'where', 'holly', 'found', 'the', 'fossil', 'which', 'is', 'in', 'the', 'middle', 'of', 'nowhere', 'with', 'cave', 'gift', 'shop', 'owner', 'will', 'stanton', 'danny', 'mcbride', 'they', 'raft', 'into', 'the', 'cave', 'where', 'marshall', 'has', 'detected', 'high', 'levels', 'of', 'tachyons', 'he', 'activates', 'the', 'tachyon', 'amplifier', 'triggering', 'an', 'earthquake', 'that', 'opens', 'a', 'time', 'warp', 'into', 'which', 'the', 'raft', 'falls', 'the', 'group', 'finds', 'themselves', 'in', 'a', 'desert', 'filled', 'with', 'various', 'items', 'from', 'many', 'eras', 'and', 'without', 'the', 'amplifier', 'they', 'rescue', 'a', 'primate', 'like', 'creature', 'cha', 'ka', 'of', 'the', 'pakuni', 'tribe', 'who', 'becomes', 'their', 'friend', 'and', 'guide', 'the', 'gang', 'spends', 'a', 'night', 'in', 'a', 'cave', 'after', 'surviving', 'a', 'meeting', 'with', 'a', 'fast', 'intelligent', 'tyrannosaurus', 'they', 'nickname', 'grumpy', 'who', 'develops', 'a', 'vendetta', 'against', 'marshall', 'for', 'calling', 'him', 'stupid', 'marshall', 'receives', 'a', 'telepathic', 'message', 'begging', 'for', 'help', 'and', 'ends', 'up', 'in', 'ancient', 'ruins', 'there', 'the', 'group', 'encounters', 'a', 'race', 'of', 'lizard', 'men', 'called', 'sleestaks', 'before', 'meeting', 'the', 'one', 'who', 'sent', 'marshall', 'the', 'telepathic', 'message', 'enik', 'the', 'altrusian', 'he', 'explains', 'that', 'he', 'was', 'exiled', 'by', 'the', 'evil', 'zarn', 'who', 'is', 'attempting', 'to', 'take', 'over', 'earth', 'with', 'his', 'sleestak', 'minions', 'but', 'enik', 'can', 'prevent', 'this', 'if', 'marshall', 'retrieves', 'the', 'tachyon', 'amplifier', 'the', 'group', 'stumble', 'upon', 'a', 'desert', 'where', 'many', 'things', 'from', 'across', 'time', 'end', 'up', 'and', 'they', 'encounter', 'many', 'compsognathus', 'velociraptors', 'grumpy', 'and', 'an', 'allosaurus', 'the', 'allosaurus', 'and', 'grumpy', 'battle', 'it', 'out', 'over', 'the', 'most', 'recent', 'thing', 'to', 'appear', 'until', 'they', 'sense', 'marshall', 'and', 'chase', 'him', 'marshall', 'kills', 'the', 'allosaurus', 'with', 'liquid', 'nitrogen', 'and', 'finds', 'that', 'the', 'amplifier', 'was', 'inside', 'the', 'allosaurus', 'the', 'amplifier', 'is', 'stolen', 'by', 'a', 'pteranodon', 'and', 'taken', 'to', 'its', 'nest', 'the', 'group', 'arrives', 'at', 'the', 'nest', 'and', 'marshall', 'lightly', 'steps', 'through', 'the', 'pteranadon', 'eggs', 'to', 'retrieve', 'the', 'amplifier', 'but', 'when', 'he', 'reaches', 'it', 'it', 'stops', 'broadcasting', 'the', 'soundtrack', 'to', 'marshalls', 'favorite', 'musical', 'a', 'chorus', 'line', 'when', 'the', 'eggs', 'begin', 'to', 'hatch', 'holly', 'realizes', 'that', 'the', 'music', 'was', 'acting', 'as', 'a', 'sort', 'of', 'lullaby', 'keeping', 'the', 'pteranadons', 'asleep', 'marshall', 'will', 'and', 'holly', 'belt', 'out', 'i', 'hope', 'i', 'get', 'it', 'with', 'cha', 'ka', 'inexplicably', 'joining', 'in', 'displaying', 'an', 'impressive', 'singing', 'voice', 'marshall', 'will', 'and', 'cha', 'ka', 'celebrate', 'their', 'good', 'fortune', 'meanwhile', 'holly', 'pockets', 'a', 'dinosaur', 'egg', 'and', 'learns', 'from', 'a', 'recording', 'left', 'by', 'the', 'long', 'deceased', 'zarn', 'that', 'enik', 'deceived', 'them', 'and', 'is', 'actually', 'the', 'one', 'planning', 'to', 'invade', 'earth', 'but', 'is', 'captured', 'by', 'the', 'sleestaks', 'to', 'be', 'brought', 'to', 'the', 'library', 'of', 'skulls', 'for', 'judgment', 'the', 'others', 'save', 'her', 'from', 'being', 'executed', 'for', 'helping', 'enik', 'but', 'the', 'villain', 'mdash', 'now', 'possessing', 'the', 'amplifier', 'and', 'mind', 'controlling', 'the', 'sleestaks', 'mdash', 'leaves', 'them', 'to', 'open', 'a', 'portal', 'to', 'earth', 'marshall', 'quickly', 'settles', 'things', 'with', 'grumpy', 'befriending', 'him', 'and', 'joins', 'the', 'others', 'to', 'defeat', 'the', 'sleestak', 'army', 'and', 'confront', 'enik', 'after', 'the', 'crystal', 'link', 'between', 'the', 'land', 'of', 'the', 'lost', 'and', 'earth', 'is', 'shattered', 'will', 'chooses', 'to', 'stay', 'behind', 'to', 'live', 'a', 'better', 'life', 'and', 'to', 'prevent', 'enik', 'from', 'following', 'marshall', 'and', 'holly', 'back', 'to', 'earth', 'learning', 'later', 'that', 'female', 'pakuni', 'are', 'very', 'attractive', 'a', 'triumphant', 'marshall', 'again', 'appears', 'on', 'today', 'with', 'show', 'the', 'dinosaur', 'egg', 'holly', 'brought', 'back', 'to', 'promote', 'his', 'new', 'book', 'matt', 'lauer', 'can', 'suck', 'it', 'however', 'left', 'behind', 'on', 'the', 'today', 'set', 'the', 'egg', 'turns', 'out', 'to', 'be', 'a', 'sleestak', 'egg', 'as', 'a', 'baby', 'hatches', 'from', 'it']\n",
      "the         48\n",
      "a           29\n",
      "and         23\n",
      "to          20\n",
      "marshall    14\n",
      "            ..\n",
      "develops     1\n",
      "vendetta     1\n",
      "against      1\n",
      "calling      1\n",
      "hatches      1\n",
      "Name: count, Length: 325, dtype: int64\n",
      "        4. Character name 6. Actor gender\n",
      "262483           Teenager               M\n",
      "262484  Dr. Rick Marshall               M\n",
      "262485     Holly Cantrell               F\n",
      "262486       Will Stanton               M\n",
      "262487             Cha-Ka               M\n",
      "262488               Enik               M\n",
      "262489                NaN               M\n",
      "262490       Tar Pits Kid               M\n",
      "262491       Tar Pits Kid               F\n",
      "262492            Teacher               M\n",
      "262493           Teenager               M\n",
      "262494          Astronaut               M\n",
      "262495           The Zarn               M\n",
      "262496              Ernie               M\n",
      "262497              Barry               M\n"
     ]
    }
   ],
   "source": [
    "movie_idx = 26\n",
    "sample_text = plot_summaries_bechdel.iloc[movie_idx][\"plot_summary\"]\n",
    "print(sample_text)\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "tokens = [x.lower() for x in tokens]\n",
    "tokens_freq = pd.Series(tokens).value_counts(sort=True)\n",
    "print(tokens)\n",
    "print(tokens_freq)\n",
    "first_movie_id = plot_summaries_bechdel.iloc[movie_idx][\"id\"]\n",
    "character_list = character_metadata_bechdel[character_metadata_bechdel['1. Wikipedia movie ID'] == first_movie_id][['4. Character name','6. Actor gender']]\n",
    "print(character_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       4. Character name 6. Actor gender\n",
      "262483          teenager               M\n",
      "262484               dr.               M\n",
      "262485             holly               F\n",
      "262486              will               M\n",
      "262487            cha-ka               M\n",
      "262488              enik               M\n",
      "262490               tar               M\n",
      "262491               tar               F\n",
      "262492           teacher               M\n",
      "262493          teenager               M\n",
      "262494         astronaut               M\n",
      "262496             ernie               M\n",
      "262497             barry               M\n",
      "       4. Character name 6. Actor gender\n",
      "262484               dr.               M\n",
      "262485             holly               F\n",
      "262486              will               M\n",
      "262487            cha-ka               M\n",
      "262488              enik               M\n",
      "262492           teacher               M\n",
      "262494         astronaut               M\n",
      "262496             ernie               M\n",
      "262497             barry               M\n",
      "holly    7\n",
      "will     4\n",
      "enik     6\n",
      "Name: count, dtype: int64\n",
      "       4. Character name 6. Actor gender\n",
      "262484               dr.               M\n",
      "262485             holly               F\n",
      "262486              will               M\n",
      "262487            cha-ka               M\n",
      "262488              enik               M\n",
      "262492           teacher               M\n",
      "262494         astronaut               M\n",
      "262496             ernie               M\n",
      "262497             barry               M\n",
      "holly    7\n",
      "will     4\n",
      "enik     6\n",
      "Name: count, dtype: int64\n",
      "heree\n",
      "[7 4 6]\n",
      "       4. Character name 6. Actor gender\n",
      "262485             holly               F\n",
      "262486              will               M\n",
      "262488              enik               M\n",
      "       character_name gender  no_mention\n",
      "262485          holly      F           7\n",
      "262486           will      M           4\n",
      "262488           enik      M           6\n"
     ]
    }
   ],
   "source": [
    "character_list_processed = character_list.copy()\n",
    "character_list_processed = character_list_processed.dropna()\n",
    "#Lowercase character names\n",
    "character_list_processed[\"4. Character name\"] = character_list_processed[\"4. Character name\"].str.lower()\n",
    "#print(character_list_processed)\n",
    "#Split name and surname to two columns\n",
    "#character_list_processed[\"4. Character name\"] = character_list_processed[\"4. Character name\"].str.split('-').str[0] #corner case\n",
    "character_list_processed[\"4. Character name\"] = character_list_processed[\"4. Character name\"].str.split(' ').str[0]\n",
    "\n",
    "\n",
    "character_list_processed = character_list_processed.drop(character_list_processed[character_list_processed[\"4. Character name\"] == \"the\"].index)\n",
    "character_gender_stacked = character_list_processed.drop_duplicates(subset='4. Character name', keep=False)\n",
    "\n",
    "print(character_list_processed)\n",
    "print(character_gender_stacked)\n",
    "#Stack two columns as rows as if surnames are different characters\n",
    "#character_list_stacked = character_list_processed[[\"4. Character name\", \"4.5. Character surname\"]].stack()\n",
    "\n",
    "#Duplicate and stack gender column so that dataframe matches\n",
    "#character_gender_stacked = pd.concat([character_list_processed[\"6. Actor gender\"],character_list_processed[\"6. Actor gender\"]])\n",
    "#character_gender_stacked = pd.concat([pd.Series(character_list_stacked.to_numpy()), pd.Series(character_gender_stacked.to_numpy())], axis = 1)\n",
    "character_gender_stacked_idx = character_gender_stacked.set_index(\"4. Character name\")\n",
    "#print(character_gender_stacked)\n",
    "#print(tokens_freq)\n",
    "#print(character_gender_stacked)\n",
    "#print(tokens_freq.index)\n",
    "#print(character_gender_stacked_idx.index.intersection(tokens_freq.index))\n",
    "#print(tokens_freq[character_gender_stacked_idx.index.intersection(tokens_freq.index)])\n",
    "\n",
    "\n",
    "tokens_intersection = tokens_freq[character_gender_stacked_idx.index.intersection(tokens_freq.index)]\n",
    "character_gender_stacked = character_gender_stacked.drop_duplicates()\n",
    "print(tokens_intersection)\n",
    "print(character_gender_stacked)\n",
    "#Get the intersection between the tokens and characters dataframe\n",
    "character_mention_freq = character_gender_stacked[character_gender_stacked[\"4. Character name\"].isin(tokens_intersection.index)]#.drop_duplicates()\n",
    "print(tokens_intersection)\n",
    "print(\"heree\")\n",
    "print(tokens_intersection.values)\n",
    "print(character_mention_freq)\n",
    "character_mention_freq[\"no_mention\"] = tokens_intersection.values#pd.DataFrame({'4. Character name':character_mention_freq[0], 'no_mention':character_mention_freq.values})\n",
    "character_mention_freq.columns = [\"character_name\", \"gender\", \"no_mention\"]\n",
    "character_list_final = character_mention_freq\n",
    "print(character_list_final)\n",
    "\n",
    "#character_list1 = character_list[character_list[\"4. Character name\"].isin(tokens_freq.index.intersection(character_list[\"4. Character name\"]).to_numpy())]\n",
    "#print(character_list1.sort_values(by=[\"4. Character name\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "#print(character_list)\n",
    "#character_list_freq_added = pd.merge(character_list, character_mention_freq, on=\"4. Character name\", how=\"left\")\n",
    "#print(character_list_freq_added)\n",
    "\n",
    "character_list_freq_added = character_list_final.groupby(['gender']).sum() \n",
    "\n",
    "if len(character_list_freq_added['no_mention'].index) != 0:\n",
    "    if character_list_freq_added['no_mention'].shape[0] == 2:\n",
    "        female_mention, male_mention = character_list_freq_added['no_mention'][0], character_list_freq_added['no_mention'][1] #groupby is alphabethic, index 0 = F\n",
    "        mention_ratio = female_mention/(female_mention + male_mention)\n",
    "    elif character_list_freq_added['no_mention'].index[0] == \"M\":\n",
    "        mention_ratio = 0.\n",
    "    elif character_list_freq_added['no_mention'].index[0] == \"F\":\n",
    "        mention_ratio = 1.\n",
    "    else:\n",
    "        mention_ratio = np.nan\n",
    "else:\n",
    "    mention_ratio = np.nan\n",
    "    \n",
    "print(mention_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
